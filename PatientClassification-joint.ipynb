{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "from gumbel_softmax import DiffPool\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader, DenseDataLoader as DenseLoader\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "import torch_geometric.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(r'./data/patient_gumbel4.pickle', 'rb') as handle:\n",
    "    patient_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatientDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(PatientDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['patient.dataset']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "    \n",
    "    def process(self):\n",
    "        \n",
    "        data_list = []\n",
    "        node_labels_dict = {'CD3p': 0, 'CD3p_CD4p': 1, 'CD8p_CD3p': 2, 'Tumor': 3, 'Stroma': 4}\n",
    "#         node_labels_dict = {'TIL':0, 'Tumor': 1, 'Stroma': 2}\n",
    "        class_num = len(node_labels_dict)\n",
    "        \n",
    "        for idx, v in enumerate(patient_dict.values()):\n",
    "            for G in v:\n",
    "                node_features = torch.LongTensor([node_labels_dict[i] for i in \n",
    "                                list(nx.get_node_attributes(G, 'cell_types').values())]).unsqueeze(1)\n",
    "                x = torch.zeros(len(G.nodes), class_num).scatter_(1, node_features, 1)\n",
    "                y = torch.LongTensor([idx])\n",
    "                edges = sorted([e for e in G.edges] + [e[::-1] for e in G.edges])\n",
    "                edge_index = torch.tensor([[e[0] for e in edges],\n",
    "                                           [e[1] for e in edges]], dtype=torch.long)\n",
    "                data = Data(x=x, edge_index=edge_index, y=y)\n",
    "                data_list.append(data)\n",
    "        \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path, sparse=False):\n",
    "    \n",
    "    dataset = PatientDataset(path)\n",
    "    if not sparse:\n",
    "        max_num_nodes = 0\n",
    "        for data in dataset:\n",
    "            max_num_nodes = max(data.num_nodes, max_num_nodes)\n",
    "\n",
    "        if dataset.transform is None:\n",
    "            dataset.transform = T.ToDense(max_num_nodes)\n",
    "        else:\n",
    "            dataset.transform = T.Compose(\n",
    "                [dataset.transform, T.ToDense(max_num_nodes)])\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = {}    \n",
    "path = './data/patient_gumbel4'\n",
    "dataset_dict['gumbel2_5'] = get_dataset(path, sparse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InclassShuffleSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, data_source, m):\n",
    "        self.data_source = data_source\n",
    "        self.m = m\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(shuffle(self.data_source, self.m))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different learning rates for a specific layer  \n",
    "https://discuss.pytorch.org/t/different-learning-rate-for-a-specific-layer/33670/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda: 0' if torch.cuda.is_available() else 'cpu')\n",
    "num_patches = 2\n",
    "total = 5\n",
    "\n",
    "def run(dataset, model, temp, total, annealing_rate, lower_bound, epochs, batch_size, \\\n",
    "        lr_base, lr, lr_decay_factor, lr_decay_step_size, weight_decay, logger=None, \\\n",
    "        resume=None, aux_loss=False, alpha1=1, alpha2=0, alpha3=0):\n",
    "    \n",
    "    lines = []\n",
    "    sampler = InclassShuffleSampler(dataset, total)\n",
    "    \n",
    "    if 'adj' in dataset[0]:\n",
    "        # This data loader only works with dense adjacency matrices\n",
    "        train_loader = DenseLoader(dataset, batch_size, shuffle=False, sampler=sampler)\n",
    "    else:\n",
    "        train_loader = DataLoader(dataset, batch_size, shuffle=False, sampler=sampler)\n",
    "        \n",
    "    # !!! optimizer  \n",
    "    parameters_list = ['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias', 'fc3.weight', 'fc3.bias']\n",
    "    parameters = [kv[1] for kv in model.named_parameters() if kv[0] in parameters_list]\n",
    "    base_parameters = [kv[1] for kv in model.named_parameters() if kv[0] not in parameters_list]\n",
    "    optimizer = Adam([{'params': base_parameters},\n",
    "                      {'params': parameters, 'lr': lr}\n",
    "                     ], lr=lr_base, weight_decay=weight_decay)\n",
    "    \n",
    "    # save on cpu, load on cpu\n",
    "    if resume:\n",
    "        last_checkpoint = torch.load(dir_path + 'checkpoint_last.pt')\n",
    "        model.load_state_dict(last_checkpoint['state_dict'])\n",
    "        model.to(device)\n",
    "        optimizer.load_state_dict(last_checkpoint['optimizer'])\n",
    "        start_epoch = last_checkpoint['epoch']+1\n",
    "    else:\n",
    "        # !!! randomly initialize parameters of the sampling module\n",
    "        model.to(device).reset_gumbel()\n",
    "        # !!! or randomly initialize all parameters if no pre-training\n",
    "#         model.to(device).reset_parameters()\n",
    "        start_epoch = 1\n",
    "        \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "            \n",
    "    # !!! save initial parameters\n",
    "    torch.save(model.state_dict(), dir_path+'params_epoch{}.pt'.format(0))\n",
    "    \n",
    "    t_start = time.perf_counter()\n",
    "    \n",
    "    for epoch in tqdm(range(start_epoch, start_epoch + epochs)):\n",
    "        train_loss, train_acc = train(model, optimizer, train_loader, initial_temp, annealing_rate, \\\n",
    "                                      lower_bound, epoch, total, aux_loss, alpha1, alpha2, alpha3)\n",
    "\n",
    "        eval_info = {\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "        }\n",
    "\n",
    "        if logger is not None:\n",
    "            lines.append(logger(eval_info))\n",
    "\n",
    "        if epoch % lr_decay_step_size == 0:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_decay_factor * param_group['lr']\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            torch.save(model.state_dict(), dir_path+'params_epoch{}.pt'.format(epoch))\n",
    "            \n",
    "    checkpoint = {'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "    torch.save(checkpoint, dir_path + 'checkpoint_last.pt')\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    t_end = time.perf_counter()\n",
    "    duration = t_end - t_start\n",
    "    lines.append('Duration: {:.3f}'.format(duration))\n",
    "    \n",
    "    return lines\n",
    "      \n",
    "\n",
    "def shuffle(dataset, m=20):\n",
    "    indices = []\n",
    "    for i in range(10):\n",
    "        tmp = [j for j in range(i*m, i*m+m)]\n",
    "        random.shuffle(tmp)\n",
    "        indices += tmp\n",
    "    return indices\n",
    "\n",
    "\n",
    "def cosine_distance(x1, x2=None, eps=1e-8):\n",
    "    x2 = x1 if x2 is None else x2\n",
    "    w1 =z x1.norm(p=2, dim=1, keepdim=True)\n",
    "    w2 = w1 if x2 is x1 else x2.norm(p=2, dim=1, keepdim=True)\n",
    "    tmp = 1 - torch.mm(x1, x2.t()) / (w1 * w2.t()).clamp(min=eps)\n",
    "    tmp = torch.triu(tmp, diagonal=1)\n",
    "    res = tmp.sum() / ((tmp.size()[0] * tmp.size()[1] - tmp.size()[1]) / 2)\n",
    "    return res\n",
    "\n",
    "def train(model, optimizer, loader, initial_temp, annealing_rate, lower_bound, epoch, \\\n",
    "          total=20, aux_loss=False, alpha1=1, alpha2=0, alpha3=0):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    base = len(loader)*(epoch-1)\n",
    "    num_patients = (len(loader.dataset)/total)\n",
    "    \n",
    "    for batch_idx, data in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        iteration = base + batch_idx\n",
    "        temp = np.maximum(initial_temp * np.exp(-annealing_rate * iteration), lower_bound)\n",
    "        if aux_loss:\n",
    "            out, discard_graphs, z = model(data, temp)\n",
    "        else:\n",
    "            out = model(data, temp)\n",
    "        len_ = len(data.y)\n",
    "        indices = [i for i in range(0, len_, total)]\n",
    "        # !!! loss function\n",
    "        if aux_loss:\n",
    "            loss = alpha1*F.nll_loss(out, data.y[indices].view(-1), reduction='sum') / num_patients + \\\n",
    "                   alpha2*F.pdist(discard_graphs, p=2).mean() / len(loader) + \\\n",
    "                   alpha3*(z[:, 0, :]*z[:, 1, :]).sum() / num_patients\n",
    "#             loss = alpha1*F.nll_loss(out, data.y[indices].view(-1), reduction='sum') / num_patients + \\\n",
    "#                    alpha2*cosine_distance(discard_graphs).mean() / len(loader)\n",
    "        else:\n",
    "            loss = F.nll_loss(out, data.y[indices].view(-1), reduction='sum') / num_patients\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        pred = out.max(1)[1]\n",
    "        correct += pred.eq(data.y[indices].view(-1)).sum().item()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return total_loss, correct / num_patients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temparature annealing schedule \n",
    "x = [i for i in range(1, 5000)]\n",
    "y = [np.maximum(10 * np.exp(-0.001 * i), 0.5) for i in x]\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%capture cap --no-stderr\n",
    "num_layers = 5\n",
    "hidden = 64\n",
    "num_hops = 2\n",
    "batch_size = 50\n",
    "ratio = 0.05\n",
    "dropout = False\n",
    "Net =  DiffPool \n",
    "initial_temp = 10\n",
    "annealing_rate = 0.001\n",
    "lower_bound = 0.5\n",
    "hard = True\n",
    "aux_loss = False\n",
    "alpha1 = 1\n",
    "alpha2 = 0\n",
    "alpha3 = 0\n",
    "lr_base = 1e-6\n",
    "lr = 1e-4\n",
    "decay_rate = 1\n",
    "\n",
    "def logger(info):\n",
    "    epoch = info['epoch']\n",
    "    train_loss, train_acc = info['train_loss'], info['train_acc']\n",
    "    output = '{:03d}: Train Loss: {:.4f}, Train Accuracy: {:.3f}'\\\n",
    "              .format(epoch, train_loss, train_acc)\n",
    "    return output\n",
    "\n",
    "dir_path = './data'+ '/' + 'DiffPool_gs_bs50_r005_joint'+ '/' + 'gumbel2_5-max-hard-add' + '/'\n",
    "# dir_path = './data' + '/' + 'DiffPool_gs_bs50_r005_joint'+ '/' + 'gumbel1_5-avg-hard' + '/'\n",
    "hyperparams_name = 'hyperparams.pickle'\n",
    "\n",
    "\n",
    "if os.path.exists(dir_path + hyperparams_name):\n",
    "    with open(r'{}'.format(dir_path + hyperparams_name), 'rb') as handle:\n",
    "        hyperparams = pickle.load(handle)\n",
    "        num_layers = hyperparams['# of layers']\n",
    "        hidden = hyperparams['# of hidden units']\n",
    "        num_hops = hyperparams['# of hops']\n",
    "        batch_size = hyperparams['batch size']\n",
    "        ratio = hyperparams['pooling ratio']\n",
    "        dropout = hyperparams['dropout']\n",
    "        num_patches = hyperparams['# of patches']\n",
    "\n",
    "\n",
    "else:\n",
    "    lines = ['-----\\{}'.format(Net.__name__)]\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    lines.append('Num of Layers: {}, Num of Hidden Units: {}, Num of Hops: {}, Batch Size: {}, ' \\\n",
    "                 'Pooling Ratio: {}, Dropout: {}, Num of Patches: {}, Total Patches: {}, ' \\\n",
    "                 'Initial temperature: {}, Annealing Rate: {}, Lower Bound: {}, ' \\\n",
    "                 'Auxiliary Loss: {}, Alpha1: {}, Alpha2: {}, Alpha3: {}, ' \\\n",
    "                 'Base Learning Rate: {}, Learning Rate: {}, Decay Rate: {}, hard:{}'\n",
    "                 .format(num_layers, hidden, num_hops, batch_size, ratio, dropout, num_patches, total, \\\n",
    "                         initial_temp, annealing_rate, lower_bound, aux_loss, \\\n",
    "                         alpha1, alpha2, alpha3, lr_base, lr, decay_rate, hard))\n",
    "    dataset = dataset_dict['gumbel2_5']\n",
    "    model = Net(dataset, num_layers, hidden, hop=num_hops, num_patches=num_patches, ratio=ratio, dropout=dropout, \\\n",
    "            total=total, hard_train=hard, aux_loss=aux_loss, decay_rate=decay_rate)\n",
    "    # !!! load pre-trainined parameters\n",
    "#     orig_dir_path = './data/DiffPool_diff_pool6_max_bs50/gumbel1_5/'\n",
    "#     params_name = 'params_epoch175.pt'\n",
    "    orig_dir_path = './data/DiffPool_diff_pool6_max_bs50/gumbel2_5/'\n",
    "    params_name = 'params_epoch140.pt'\n",
    "    state = model.state_dict()\n",
    "    params = torch.load(orig_dir_path+params_name, map_location=device)\n",
    "    state.update(params)\n",
    "    model.load_state_dict(state)\n",
    "    \n",
    "    process_lines = \\\n",
    "    run(\n",
    "        dataset,\n",
    "        model,\n",
    "        initial_temp,\n",
    "        total,\n",
    "        annealing_rate,\n",
    "        lower_bound,\n",
    "        epochs=5000,\n",
    "        batch_size=batch_size,\n",
    "        lr_base=lr_base,\n",
    "        lr=lr,\n",
    "        lr_decay_factor=0.5,\n",
    "        lr_decay_step_size=50,\n",
    "        weight_decay=0,\n",
    "        logger=logger,\n",
    "        aux_loss=aux_loss,\n",
    "        alpha1=alpha1,\n",
    "        alpha2=alpha2,\n",
    "        alpha3=alpha3\n",
    "    )\n",
    "    lines += process_lines\n",
    "\n",
    "    hyperparams = {'# of layers': num_layers, '# of hidden units': hidden, '# of hops': num_hops, \\\n",
    "                   'batch size': batch_size, 'pooling ratio': ratio, 'dropout':dropout, '# of patches': num_patches}\n",
    "\n",
    "    with open(r'{}'.format(dir_path + hyperparams_name), 'wb') as handle:\n",
    "        pickle.dump(hyperparams, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    now = datetime.now()\n",
    "    date_time = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    filename = 'log_' + date_time + '.txt'\n",
    "    logfile = open(dir_path + filename, 'w')\n",
    "    for line in lines:\n",
    "        logfile.write(\"{}\\n\".format(line))\n",
    "    logfile.close()\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (geodeep)",
   "language": "python",
   "name": "geodeep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
